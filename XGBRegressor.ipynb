{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "We will make extensive use of `pandas`, `XGBoost` and it's `scikit-learn` API throughout this demo. `pickle` will be used to save and load model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Slack channel notifications\n",
    "\n",
    "Import `SlackClient` and create basic function that will post a Slack notification in `channel` when code is finished running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from slackclient import SlackClient\n",
    "def slack_message(message, channel):\n",
    "    token = 'your_token'\n",
    "    sc = SlackClient(token)\n",
    "    sc.api_call('chat.postMessage', channel=channel, \n",
    "                text=message, username='My Sweet Bot',\n",
    "                icon_emoji=':upside_down_face:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import data and set data types\n",
    "\n",
    "Set working directory and ensure schema is correct before importing train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/your/directory/'  \n",
    "data_file = data_dir + 'data_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file, sep=\"\\t\", parse_dates = ['dates'], date_parser = pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess data frames\n",
    "\n",
    "Parse through `replaceValues` in order to standardise character strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def replaceValues(df):\n",
    "    df.replace(r'[\\s]','_', inplace = True, regex = True)\n",
    "    df.replace(r'[\\.]','', inplace = True, regex = True)\n",
    "    df.replace(r'__','_', inplace = True, regex = True)\n",
    "\n",
    "replaceValues(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Combine train and test set\n",
    "\n",
    "Combine `train` and `test` data sets before parsing through one-hot encoder or dense vector encoding. This is especially important for one-hot encoding because we want to maintain the same set of columns across both train and test sets. These can be inconsistent if a particular level of a categorical variable is present in one data set but not the other\n",
    "\n",
    "* `cat_cols` are categorical columns that will be used in model training\n",
    "* `index_cols` are the index columns of the dataframe, which will not be used in model training\n",
    "* `pred_cols` are the response variable columns\n",
    "* `num_cols` are the numeric columns that will be used in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['ATTRIBUTE_1','ATTRIBUTE_2','ATTRIBUTE_3']\n",
    "index_cols = ['FACTOR_1','FACTOR_2','FACTOR_3']\n",
    "pred_cols = ['RESPONSE']\n",
    "\n",
    "num_cols = [x for x in list(data.columns.values) if x not in cat_cols if x not in fac_cols if x not in pred_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## To one-hot encode the categorical variables, run the next cell. To code categorial variables as a dense vector, run the next cell instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def categoricalCols(indf, cat_var_list):\n",
    "#     for cv in cat_var_list:\n",
    "#         if [i for i, x in enumerate(cat_var_list) if cv == x][0] == 0:\n",
    "#             dummy_df = pd.get_dummies(indf[cv], prefix = cv)\n",
    "#         else:\n",
    "#             dummy_df = pd.concat([dummy_df, pd.get_dummies(indf[cv], prefix = cv)], axis = 1)\n",
    "#     return dummy_df\n",
    "        \n",
    "# combined_cat = categoricalCols(combined[cat_cols], cat_cols)\n",
    "# combined_cat.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_cat = pd.DataFrame(data[cat_cols])\n",
    "for feature in cat_cols: # Loop through all columns in the dataframe\n",
    "    if data_cat[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        data_cat[feature] = pd.Categorical(data[feature]).codes # Replace strings with an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare final dataframe before resplitting into train and test sets\n",
    "\n",
    "Importantly, we want to ensure that `train_final` and `test_final` are the same rows of data as `train` and `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_num = data[num_cols]\n",
    "data_final = pd.concat([data_cat, data_num], axis=1)\n",
    "data_final['DATE'] = data['DATE']\n",
    "data_final['RESPONSE'] = data['RESPONSE']\n",
    "print data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_final = data_final[data_final['DATE'] <= 'DATE_SPLIT']\n",
    "test_final = data_final[data_final['DATE'] >= 'DATE_SPLIT' ]\n",
    "\n",
    "print(train_final.shape)\n",
    "print(test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = data[data['DATE'] <= 'DATE_SPLIT']\n",
    "test = data[data['DATE'] >= 'DATE_SPLIT' ]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create design matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train = train_final['RESPONSE']\n",
    "y_test = test_final['RESPONSE']\n",
    "x_train = train_final.drop(['RESPONSE','DATE'], axis=1)\n",
    "x_test = test_final.drop(['RESPONSE','DATE'], axis=1)\n",
    "\n",
    "print x_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Begin parameter tuning for XGBoost\n",
    "\n",
    "First, we tune the `max_depth` and `min_child_weight` parameters on a wide range of values. Later, we will refine these two choices with a smaller grid. Note that if you are running this in a Jupyter notebook, you can see the training process in your bash window. We will use the `parameters` dict to store the latest parameter values, and the `scores` vector to store the MSE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "objective = \"reg:linear\"\n",
    "seed = 100\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "gamma = 0.1\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "reg_alpha = 1\n",
    "reg_lambda = 1\n",
    "silent = False\n",
    "\n",
    "parameters = {}\n",
    "parameters['objective'] = objective\n",
    "parameters['seed'] = seed\n",
    "parameters['n_estimators'] = n_estimators\n",
    "parameters['learning_rate'] = learning_rate\n",
    "parameters['gamma'] = gamma\n",
    "parameters['colsample_bytree'] = colsample_bytree\n",
    "parameters['reg_alpha'] = reg_alpha\n",
    "parameters['reg_lambda'] = reg_lambda\n",
    "parameters['silent'] = silent\n",
    "\n",
    "scores = []\n",
    "\n",
    "cv_params = {'max_depth': [2,4,6,8],\n",
    "             'min_child_weight': [1,3,5,7]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                    \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"max_depth and min_child_weight parameters tuned! moving on to refinement\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Refine with a smaller grid of values based on best values from the big grid above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_depth = gbm.best_params_['max_depth']\n",
    "min_child_weight = gbm.best_params_['min_child_weight']\n",
    "parameters['max_depth'] = max_depth\n",
    "parameters['min_child_weight'] = min_child_weight\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'max_depth': [max_depth-1, max_depth, max_depth+1], \n",
    "             'min_child_weight': [min_child_weight-1, min_child_weight-0.5, min_child_weight, min_child_weight+0.5, min_child_weight+1]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"max_depth and min_child_weight parameters refined! moving on to tuning gamma parameter\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set max_depth and min_child_weight before tuning gamma parameter\n",
    "\n",
    "Set the `max_depth` and `min_child_weight` values based on the above before tuning the `gamma` parameter in a similar fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_depth = gbm.best_params_['max_depth']\n",
    "min_child_weight = gbm.best_params_['min_child_weight']\n",
    "parameters['max_depth'] = max_depth\n",
    "parameters['min_child_weight'] = min_child_weight\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'gamma': [i/10.0 for i in range(1,10,2)]}\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"gamma tuned! moving on to tuning subsample and colsample_bytree parameters\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set the `gamma` parameter and tune the `subsample` and `colsample_bytree` parameters next \n",
    "\n",
    "We will look at 10% intervals from 60% to 100% for both `subsample` and `colsample_bytree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gamma = gbm.best_params_['gamma']\n",
    "parameters['gamma'] = gamma\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'subsample': [i/10.0 for i in range(6,11)],\n",
    "             'colsample_bytree': [i/10.0 for i in range(6,11)]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"subsample and colsample_bytree parameters tuned! moving on to refinement\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retune with a smaller grid of values based on best values from the big grid above\n",
    "\n",
    "Look at 5% intervals in some range around the best values found previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "subsample = gbm.best_params_['subsample']\n",
    "colsample_bytree = gbm.best_params_['colsample_bytree']\n",
    "parameters['subsample'] = subsample\n",
    "parameters['colsample_bytree'] = colsample_bytree\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'subsample': [i/100.0 for i in range(int((subsample-0.1)*100.0), min(int((subsample+0.1)*100),105) , 5)],\n",
    "             'colsample_bytree': [i/100.0 for i in range(int((colsample_bytree-0.1)*100.0), min(int((subsample+0.1)*100),105), 5)]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"subsample and colsample_bytree parameters refined! moving on to tuning the alpha and lambda parameters\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set the `colsample_bytree` and `subsample` parameters before tuning `reg_alpha` and `reg_lambda` parameters\n",
    "\n",
    "`reg_alpha` controls L1 regularisation and `reg_lambda` controls L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = gbm.best_params_['colsample_bytree']\n",
    "subsample = gbm.best_params_['subsample']\n",
    "parameters['colsample_bytree'] = colsample_bytree\n",
    "parameters['subsample'] = subsample\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100], \n",
    "             'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        subsample = subsample,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"alpha and lambda parameters tuned! moving on to refinement\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Refine parameters on a smaller grid\n",
    "\n",
    "Look at a smaller grid around the best values found previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reg_alpha = gbm.best_params_['reg_alpha']\n",
    "reg_lambda = gbm.best_params_['reg_lambda']\n",
    "parameters['reg_alpha'] = reg_alpha\n",
    "parameters['reg_lambda'] = reg_lambda\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'reg_lambda': [reg_alpha*0.2, reg_alpha*0.5, reg_alpha, reg_alpha*2, reg_alpha*5], \n",
    "             'reg_alpha': [reg_lambda*0.2, reg_lambda*0.5, reg_lambda, reg_lambda*2, reg_lambda*5]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = objective,\n",
    "                                        seed = seed,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        subsample = subsample,\n",
    "                                        silent = silent\n",
    "\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    iid = False,\n",
    "                    scoring = \"neg_mean_squared_error\",\n",
    "                    cv = 5,\n",
    "                    verbose = True\n",
    ")\n",
    "\n",
    "gbm.fit(x_train,y_train)\n",
    "print gbm.cv_results_\n",
    "print \"Best parameters %s\" %gbm.best_params_\n",
    "print \"Best score %s\" %gbm.best_score_\n",
    "slack_message(\"alpha and lambda parameters refined! finalising model by reducing learning rate and increasing trees\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set regularisation parameters before increasing the number of trees and reducing the learning rate\n",
    "\n",
    "The idea here is to find a better fit that actually converges based on the optimal parameters values we have found so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reg_alpha = gbm.best_params_['reg_alpha']\n",
    "reg_lambda = gbm.best_params_['reg_lambda']\n",
    "parameters['reg_alpha'] = reg_alpha\n",
    "parameters['reg_lambda'] = reg_lambda\n",
    "scores.append(gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Print final parameters used and scores obtained\n",
    "\n",
    "Importantly, ensure scores are increasing with each iteration. For the above implementation, the negative MSE objective function should be increasing in order to minimise MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print parameters\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# n_estimators = 3000\n",
    "# learning_rate = 0.05\n",
    "\n",
    "# parameters['n_estimators'] = n_estimators\n",
    "# parameters['learning_rate'] = learning_rate\n",
    "\n",
    "# xgbFinal = xgb.XGBRegressor(\n",
    "#     objective = objective,\n",
    "#     seed = seed,\n",
    "#     n_estimators = n_estimators,\n",
    "#     max_depth = max_depth,\n",
    "#     min_child_weight = min_child_weight,\n",
    "#     learning_rate = learning_rate,\n",
    "#     gamma = gamma,\n",
    "#     subsample = subsample,\n",
    "#     colsample_bytree = colsample_bytree,\n",
    "#     reg_alpha = reg_alpha,\n",
    "#     reg_lambda = reg_lambda,\n",
    "#     silent = False\n",
    "# )\n",
    "\n",
    "# xgb1.fit(x_train, y_train, eval_set = [(x_train, y_train), (x_test, y_test)], eval_metric = 'rmse', verbose = True)\n",
    "# slack_message(\"Training complete!\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create XGBoost's DMatrix\n",
    "\n",
    "We will use this for finding the best tree via cross validation, and in the final XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = x_train, label = y_train)\n",
    "testDMat = xgb.DMatrix(data = x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Find best tree\n",
    "\n",
    "Lower the `learning_rate` and set a large `num_boost_round` hyperparameter to ensure convergence. If convergence is slow, retry with a slightly higher learning rate (e.g. `0.075` instead of `0.05`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "parameters['eta'] = learning_rate\n",
    "\n",
    "num_boost_round = 3000\n",
    "early_stopping_rounds = 20\n",
    "\n",
    "xgbCV = xgb.cv(\n",
    "    params = parameters, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 5,\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,\n",
    "    seed = seed     \n",
    ")\n",
    "\n",
    "slack_message(\"Training complete! Producing final booster object\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Finalise XGBoost model\n",
    "\n",
    "Produce the final booster object using the best tree from our cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "parameters['eval_metric'] = 'rmse'\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = parameters, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    evals = [(trainDMat, 'train'), \n",
    "             (testDMat, 'eval')]\n",
    ")\n",
    "\n",
    "slack_message(\"Booster object created!\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature importance plot\n",
    "\n",
    "Plot the feature importance plot to check whether this is making sense before checking optimal parameters and loss function progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgbFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Produce predictions for train and test sets before measuring accuracy\n",
    "\n",
    "Calculate predictions for both train and test sets, and then calculate MSE and RMSE for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgbFinal_train_preds = xgbFinal.predict(x_train)\n",
    "xgbFinal_test_preds = xgbFinal.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(xgbFinal_train_preds.shape)\n",
    "print(xgbFinal_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"\\nModel Report\"\n",
    "print \"MSE Train : %f\" % mean_squared_error(y_train, xgbFinal_train_preds)\n",
    "print \"MSE Test: %f\" % mean_squared_error(y_test, xgbFinal_test_preds)\n",
    "print \"RMSE Train: %f\" % mean_squared_error(y_train, xgbFinal_train_preds)**0.5\n",
    "print \"RMSE Test: %f\" % mean_squared_error(y_test, xgbFinal_test_preds)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save xgb model file and write .csv files to working directory\n",
    "\n",
    "Save xgb model file for future reference. Similar function to load previously saved files is commented out below. Then, write all files to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# xgb1 = pickle.load(open(\"xgb1.pickle.dat\", \"rb\"))\n",
    "# xgb1_train_preds = xgb1.predict(x_train)\n",
    "# xgb1_test_preds = xgb1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print \"\\nModel Report\"\n",
    "# print \"MSE Train : %f\" % mean_squared_error(y_train, xgb1_train_preds)\n",
    "# print \"MSE Test: %f\" % mean_squared_error(y_test, xgb1_test_preds)\n",
    "# print \"RMSE Train: %f\" % mean_squared_error(y_train, xgb1_train_preds)**0.5\n",
    "# print \"RMSE Test: %f\" % mean_squared_error(y_test, xgb1_test_preds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_preds = pd.DataFrame(xgbFinal_train_preds)\n",
    "test_preds = pd.DataFrame(xgbFinal_test_preds)\n",
    "train_preds.columns = ['RESPONSE']\n",
    "test_preds.column = ['RESPONSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('XGBoost Train.csv', sep=',')\n",
    "train_preds.to_csv('XGBoost Train Preds.csv', sep=',')\n",
    "test.to_csv('XGBoost Test.csv', sep=',')\n",
    "test_preds.to_csv('XGBoost Test Preds.csv', sep=',')\n",
    "slack_message(\"Files saved!\", 'channel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
